{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1154294/3013829255.py:24: AVDeprecationWarning: VideoStream.rate is deprecated as it is not always set; please use VideoStream.average_rate.\n",
      "  output_stream = output_container.add_stream('mpeg4', rate=stream.rate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved perturbated video for: _3qg-oAr7b0.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: GrjJSOrgM1Y.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: mAbQBmcYJkk.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: cMVEIKYF7Ps.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: pkUx0wFHMLs.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: q7z_JwcIfno.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: aia3TMX-YE0.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: 1BZimTjp8CE.mp4 with noise level 0.01\n",
      "Processed and saved perturbated video for: SpaKDNIDpQs.mp4 with noise level 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m video_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/videos_val\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust this to your actual video directory path\u001b[39;00m\n\u001b[1;32m     59\u001b[0m noise_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Adjust this value to control the strength of the shot noise\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mprocess_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(video_directory, noise_level)\u001b[0m\n\u001b[1;32m     52\u001b[0m video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_directory, video_file)\n\u001b[1;32m     53\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(perturbated_dir, video_file)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed and saved perturbated video for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with noise level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path, output_path, noise_level)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m container\u001b[38;5;241m.\u001b[39mdecode(stream):\n\u001b[1;32m     30\u001b[0m     img \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mto_image()\n\u001b[0;32m---> 31\u001b[0m     noisy_img \u001b[38;5;241m=\u001b[39m \u001b[43madd_shot_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_level\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the specified noise level\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     frame \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mVideoFrame\u001b[38;5;241m.\u001b[39mfrom_image(noisy_img)\n\u001b[1;32m     33\u001b[0m     packet \u001b[38;5;241m=\u001b[39m output_stream\u001b[38;5;241m.\u001b[39mencode(frame)\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36madd_shot_noise\u001b[0;34m(image, scale)\u001b[0m\n\u001b[1;32m     10\u001b[0m max_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(data)  \u001b[38;5;66;03m# Get maximum value to scale noise correctly\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Add Poisson noise based on the current data level\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m noisy_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoisson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m scale\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Normalize noisy data based on the original maximum value\u001b[39;00m\n\u001b[1;32m     14\u001b[0m noisy_data \u001b[38;5;241m=\u001b[39m (noisy_data \u001b[38;5;241m/\u001b[39m noisy_data\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m*\u001b[39m max_val\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:3674\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.poisson\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:114\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.int64_to_long\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/numpy/core/numeric.py:1855\u001b[0m, in \u001b[0;36misscalar\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_frombuffer\u001b[39m(buf, dtype, shape, order):\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(buf, dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mreshape(shape, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m-> 1855\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misscalar\u001b[39m(element):\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;124;03m    Returns True if the type of `element` is a scalar type.\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1929\u001b[0m \n\u001b[1;32m   1930\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(element, generic)\n\u001b[1;32m   1932\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(element) \u001b[38;5;129;01min\u001b[39;00m ScalarType\n\u001b[1;32m   1933\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(element, numbers\u001b[38;5;241m.\u001b[39mNumber))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def add_shot_noise(image, scale=1.0):\n",
    "    \"\"\"Add shot noise to an image with adjustable intensity.\"\"\"\n",
    "    # Convert PIL Image to NumPy array for processing\n",
    "    data = np.array(image).astype(np.float32)\n",
    "    max_val = np.max(data)  # Get maximum value to scale noise correctly\n",
    "    # Add Poisson noise based on the current data level\n",
    "    noisy_data = np.random.poisson(data * scale) / scale\n",
    "    # Normalize noisy data based on the original maximum value\n",
    "    noisy_data = (noisy_data / noisy_data.max()) * max_val\n",
    "    noisy_data = np.clip(noisy_data, 0, 255)  # Ensure the pixel values are valid\n",
    "    return Image.fromarray(noisy_data.astype(np.uint8))\n",
    "\n",
    "\n",
    "def process_video(video_path, output_path, noise_level):\n",
    "    \"\"\"Process a video file, apply shot noise with specified level, and save the output.\"\"\"\n",
    "    container = av.open(video_path)\n",
    "    output_container = av.open(output_path, mode='w')\n",
    "    stream = container.streams.video[0]\n",
    "    output_stream = output_container.add_stream('mpeg4', rate=stream.rate)\n",
    "    output_stream.width = stream.width\n",
    "    output_stream.height = stream.height\n",
    "    output_stream.pix_fmt = 'yuv420p'\n",
    "\n",
    "    for frame in container.decode(stream):\n",
    "        img = frame.to_image()\n",
    "        noisy_img = add_shot_noise(img, scale=noise_level)  # Use the specified noise level\n",
    "        frame = av.VideoFrame.from_image(noisy_img)\n",
    "        packet = output_stream.encode(frame)\n",
    "        if packet:\n",
    "            output_container.mux(packet)\n",
    "\n",
    "    # Finalize the file\n",
    "    for packet in output_stream.encode():\n",
    "        output_container.mux(packet)\n",
    "\n",
    "    output_container.close()\n",
    "    container.close()\n",
    "\n",
    "def process_directory(video_directory, noise_level):\n",
    "    base_directory = os.path.dirname(video_directory)\n",
    "    perturbated_dir = os.path.join(base_directory, \"perturbated\")\n",
    "    if not os.path.exists(perturbated_dir):\n",
    "        os.makedirs(perturbated_dir)\n",
    "\n",
    "    video_files = [f for f in os.listdir(video_directory) if f.endswith('.mp4')]\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_directory, video_file)\n",
    "        output_path = os.path.join(perturbated_dir, video_file)\n",
    "        process_video(video_path, output_path, noise_level)\n",
    "        print(f\"Processed and saved perturbated video for: {video_file} with noise level {noise_level}\")\n",
    "\n",
    "# Define your video directory path and desired noise level here\n",
    "video_directory = 'archive/videos_val'  # Adjust this to your actual video directory path\n",
    "noise_level = 0.01  # Adjust this value to control the strength of the shot noise\n",
    "process_directory(video_directory, noise_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7664c3e987e4658af9e7c5ed57ab3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing videos:   0%|          | 0/60 [00:00<?, ?video/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 24, 14, 8, 768]' is invalid for input of size 2107392",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m     true_labels\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;241m*\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples_per_class\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# 获取数据和预测\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m video_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshap_calculator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampled_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_exact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_exact\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# 打印并保存结果\u001b[39;00m\n\u001b[1;32m    171\u001b[0m save_results(video_data)\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mprocess_videos\u001b[0;34m(video_processor, shap_calculator, sampled_files, true_labels, use_exact)\u001b[0m\n\u001b[1;32m     91\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_directory\u001b[39m\u001b[38;5;124m\"\u001b[39m], video_file)\n\u001b[1;32m     92\u001b[0m container \u001b[38;5;241m=\u001b[39m av\u001b[38;5;241m.\u001b[39mopen(file_path)\n\u001b[0;32m---> 93\u001b[0m segment_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_video_and_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m video_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mstack([output[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m segment_outputs]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m video_pred_label \u001b[38;5;241m=\u001b[39m video_probs\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m, in \u001b[0;36mVideoProcessor.predict_video_and_segments\u001b[0;34m(self, container, true_label)\u001b[0m\n\u001b[1;32m     43\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor(\u001b[38;5;28mlist\u001b[39m(segment), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     47\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/timesformer/modeling_timesformer.py:773\u001b[0m, in \u001b[0;36mTimesformerForVideoClassification.forward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03meating spaghetti\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    771\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimesformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    782\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/timesformer/modeling_timesformer.py:639\u001b[0m, in \u001b[0;36mTimesformerModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    635\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    637\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m--> 639\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/timesformer/modeling_timesformer.py:446\u001b[0m, in \u001b[0;36mTimesformerEncoder.forward\u001b[0;34m(self, hidden_states, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    440\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    441\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    442\u001b[0m         hidden_states,\n\u001b[1;32m    443\u001b[0m         output_attentions,\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 446\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mmxai/lib/python3.12/site-packages/transformers/models/timesformer/modeling_timesformer.py:352\u001b[0m, in \u001b[0;36mTimesformerLayer.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdivided_space_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# Temporal\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     temporal_embedding \u001b[38;5;241m=\u001b[39m hidden_states[:, \u001b[38;5;241m1\u001b[39m:, :]\n\u001b[0;32m--> 352\u001b[0m     temporal_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patch_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patch_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m num_patch_height \u001b[38;5;241m*\u001b[39m num_patch_width, num_frames, temporal_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    356\u001b[0m     temporal_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_attention(\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_layernorm(temporal_embedding),\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m temporal_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 24, 14, 8, 768]' is invalid for input of size 2107392"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import json\n",
    "from transformers import AutoImageProcessor, TimesformerForVideoClassification\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, model_name, image_processor_name, device='cuda'):\n",
    "        self.model = self.load_model(model_name)\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(image_processor_name)\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        if \"timesformer\" in model_name.lower():\n",
    "            return TimesformerForVideoClassification.from_pretrained(model_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model name: {model_name}\")\n",
    "\n",
    "    def split_video_into_segments(self, container, n_segments=8, frames_per_segment=16):\n",
    "        frame_list = [frame.to_image() for frame in container.decode(video=0)]\n",
    "        total_frames = len(frame_list)\n",
    "        segment_length = total_frames // n_segments\n",
    "        segments = []\n",
    "        for i in range(n_segments):\n",
    "            start = i * segment_length\n",
    "            end = min(start + segment_length, total_frames)\n",
    "            segment_frames = frame_list[start:end] if end - start == segment_length else frame_list[start:] + [frame_list[-1]] * (segment_length - (end - start))\n",
    "            segments.append(segment_frames[:frames_per_segment])\n",
    "        return segments\n",
    "\n",
    "    def predict_video_and_segments(self, container, true_label):\n",
    "        video_segments = self.split_video_into_segments(container)\n",
    "        segment_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for segment in video_segments:\n",
    "                inputs = self.image_processor(list(segment), return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                pred_label = logits.argmax(-1).item()\n",
    "                pred_score = probabilities[0, pred_label].item()\n",
    "                segment_outputs.append((pred_label, pred_score, probabilities))\n",
    "        return segment_outputs\n",
    "\n",
    "class TemporalShap:\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def approximate_shapley_values(self, segment_outputs, label_index):\n",
    "        n = len(segment_outputs)\n",
    "        shapley_values = [0] * n\n",
    "        for _ in range(self.num_samples):\n",
    "            random_subset = sorted(range(n), key=lambda _: random.random())\n",
    "            subset_prob = torch.zeros_like(segment_outputs[0][2])\n",
    "            for i, index in enumerate(random_subset):\n",
    "                old_contribution = subset_prob[0, label_index].item()\n",
    "                subset_prob += segment_outputs[index][2]\n",
    "                subset_prob /= (i + 1)\n",
    "                new_contribution = subset_prob[0, label_index].item()\n",
    "                shapley_values[index] += new_contribution - old_contribution\n",
    "        return [val / self.num_samples for val in shapley_values]\n",
    "\n",
    "    def exact_shapley_values(self, segment_outputs, label_index):\n",
    "        n = len(segment_outputs)\n",
    "        shapley_values = [0] * n\n",
    "        all_indices = list(range(n))\n",
    "        for i in all_indices:\n",
    "            marginal_contributions = []\n",
    "            for subset_size in range(n):\n",
    "                subsets = list(combinations([x for x in all_indices if x != i], subset_size))\n",
    "                for subset in subsets:\n",
    "                    subset_prob = torch.zeros_like(segment_outputs[0][2])\n",
    "                    if subset:\n",
    "                        subset_prob = torch.mean(torch.stack([segment_outputs[j][2] for j in subset]), dim=0)\n",
    "                    with_i_prob = (subset_prob * len(subset) + segment_outputs[i][2]) / (len(subset) + 1)\n",
    "                    marginal_contributions.append(with_i_prob[0, label_index].item() - subset_prob[0, label_index].item())\n",
    "            shapley_values[i] = np.mean(marginal_contributions)\n",
    "        return shapley_values\n",
    "\n",
    "def process_videos(video_processor, shap_calculator, sampled_files, true_labels, use_exact=False):\n",
    "    predictions = []\n",
    "    for video_file, true_label in tqdm(zip(sampled_files, true_labels), desc=\"Processing videos\", total=len(sampled_files), unit=\"video\"):\n",
    "        file_path = os.path.join(config[\"video_directory\"], video_file)\n",
    "        container = av.open(file_path)\n",
    "        segment_outputs = video_processor.predict_video_and_segments(container, true_label)\n",
    "        video_probs = torch.mean(torch.stack([output[2] for output in segment_outputs]), dim=0)\n",
    "        video_pred_label = video_probs.argmax().item()\n",
    "        video_pred_score = video_probs[0, video_pred_label].item()\n",
    "        video_true_score = video_probs[0, true_label].item()\n",
    "        \n",
    "        if use_exact:\n",
    "            sv_true_label = shap_calculator.exact_shapley_values(segment_outputs, true_label)\n",
    "            sv_video_pred = shap_calculator.exact_shapley_values(segment_outputs, video_pred_label)\n",
    "        else:\n",
    "            sv_true_label = shap_calculator.approximate_shapley_values(segment_outputs, true_label)\n",
    "            sv_video_pred = shap_calculator.approximate_shapley_values(segment_outputs, video_pred_label)\n",
    "        \n",
    "        predictions.append((video_file, video_pred_label, video_pred_score, video_true_score, segment_outputs, sv_true_label, sv_video_pred))\n",
    "    return predictions\n",
    "\n",
    "def save_results(predictions, filename=\"results.json\"):\n",
    "    results = []\n",
    "    for video_file, video_pred_label, video_pred_score, video_true_score, segment_outputs, sv_true_label, sv_video_pred in predictions:\n",
    "        video_result = {\n",
    "            \"video_file\": video_file,\n",
    "            \"video_pred_label\": video_pred_label,\n",
    "            \"video_pred_score\": video_pred_score,\n",
    "            \"video_true_score\": video_true_score,\n",
    "            \"segments\": []\n",
    "        }\n",
    "        for i, (segment_label, segment_score, probabilities) in enumerate(segment_outputs):\n",
    "            segment_video_label_score = probabilities[0, video_pred_label].item()\n",
    "            segment_true_label_score = probabilities[0, true_labels[predictions.index((video_file, video_pred_label, video_pred_score, video_true_score, segment_outputs, sv_true_label, sv_video_pred))]].item()  # 使用当前 video 的 true_label\n",
    "            video_result[\"segments\"].append({\n",
    "                \"segment_index\": i + 1,\n",
    "                \"segment_label\": segment_label,\n",
    "                \"segment_score\": segment_score,\n",
    "                \"segment_video_label_score\": segment_video_label_score,\n",
    "                \"segment_true_label_score\": segment_true_label_score,\n",
    "                \"sv_true_label\": sv_true_label[i],\n",
    "                \"sv_video_pred\": sv_video_pred[i]\n",
    "            })\n",
    "        results.append(video_result)\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "# 配置\n",
    "config = {\n",
    "    \"model_name\": \"facebook/timesformer-base-finetuned-k400\",  # 用户可以在这里更换模型名称，例如 \"huggingface/vivit\"\n",
    "    \"image_processor_name\": \"MCG-NJU/videomae-base-finetuned-kinetics\",\n",
    "    \"num_samples\": 100,  # 近似 Shapley Value 计算的采样次数\n",
    "    \"num_classes\": 20,    # 要测试的类的数量\n",
    "    \"num_samples_per_class\": 3,  # 每个类的样本数量\n",
    "    \"video_list_path\": \"archive/kinetics400_val_list_videos.txt\",\n",
    "    \"video_directory\": \"archive/videos_val\",\n",
    "    \"use_exact\": True  # 设置为 True 以使用精确 Shapley Value 计算\n",
    "}\n",
    "\n",
    "# 初始化处理器\n",
    "video_processor = VideoProcessor(config[\"model_name\"], config[\"image_processor_name\"])\n",
    "shap_calculator = TemporalShap(num_samples=config[\"num_samples\"])\n",
    "\n",
    "# 读取视频列表和标签，并按类别组织\n",
    "video_labels = defaultdict(list)\n",
    "with open(config[\"video_list_path\"], \"r\") as f:\n",
    "    for line in f:\n",
    "        name, label = line.strip().split()\n",
    "        video_labels[int(label)].append(name)\n",
    "\n",
    "# 准备视频样本\n",
    "sampled_files = []\n",
    "true_labels = []\n",
    "selected_classes = random.sample(list(video_labels.keys()), config[\"num_classes\"])  # 随机选择指定数量的类别\n",
    "for cls in selected_classes:\n",
    "    sampled_files.extend(random.sample(video_labels[cls], config[\"num_samples_per_class\"]))  # 从每个类别中随机选择指定数量的样本\n",
    "    true_labels.extend([cls] * config[\"num_samples_per_class\"])\n",
    "\n",
    "# 获取数据和预测\n",
    "video_data = process_videos(video_processor, shap_calculator, sampled_files, true_labels, use_exact=config[\"use_exact\"])\n",
    "\n",
    "# 打印并保存结果\n",
    "save_results(video_data)\n",
    "\n",
    "# 打印结果以便查看\n",
    "for video_file, video_pred_label, video_pred_score, video_true_score, segment_outputs, sv_true_label, sv_video_pred in video_data:\n",
    "    true_label = true_labels[sampled_files.index(video_file)]\n",
    "    print(f\"Video: {video_file}, Overall Predicted Label = {video_pred_label}, Overall Prediction Score = {video_pred_score:.4f}, True Label = {true_label}, True Label Score = {video_true_score:.4f}\")\n",
    "    for i, (segment_label, segment_score, probabilities) in enumerate(segment_outputs):\n",
    "        segment_video_label_score = probabilities[0, video_pred_label].item()\n",
    "        segment_true_label_score = probabilities[0, true_label].item()\n",
    "        print(f\"  Segment {i+1}: Predicted Label = {segment_label}, Prediction Score = {segment_score:.4f}, Segment Video Label Score = {segment_video_label_score:.4f}, Segment True Label Score = {segment_true_label_score:.4f}, SV True Label = {sv_true_label[i]:.4f}, SV Predicted Label = {sv_video_pred[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmxai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
